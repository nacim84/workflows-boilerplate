# Sp√©cifications Techniques - Workflows Agentiques Python

Ce document d√©finit les sp√©cifications techniques, les frameworks et les bonnes pratiques pour construire des workflows agentiques robustes et √©volutifs avec Python.

## üéØ Objectif du Projet

Cr√©er une alternative **by-code** aux plateformes no-code comme N8N, en s'appuyant sur :
- Des agents IA autonomes et collaboratifs
- Un √©cosyst√®me complet d'outils et MCPs
- Une architecture √† 3 couches (Directive ‚Üí Orchestration ‚Üí Ex√©cution)
- Des workflows reproductibles, testables et maintenables

---

## üìã Table des Mati√®res

1. [Stack Technique Core](#1-stack-technique-core)
2. [Frameworks Agentiques](#2-frameworks-agentiques)
3. [Model Context Protocol (MCP)](#3-model-context-protocol-mcp)
4. [LLM Providers & Int√©grations](#4-llm-providers--int√©grations)
5. [Infrastructure & Outils](#5-infrastructure--outils)
6. [Patterns Agentiques](#6-patterns-agentiques)
7. [Observabilit√© & Monitoring](#7-observabilit√©--monitoring)
8. [Migration N8N ‚Üí Code](#8-migration-n8n--code)
9. [Standards de D√©veloppement](#9-standards-de-d√©veloppement)

---

## 1. Stack Technique Core

### Python Version
- **Minimum requis** : Python 3.11+
- **Recommand√©** : Python 3.12+ pour meilleures performances
- **Raisons** :
  - Support natif des types g√©n√©riques am√©lior√©s
  - Meilleures performances asyncio
  - Pattern matching (match/case)
  - Better error messages

### Gestionnaire de Paquets : UV (Astral)

**UV** est le gestionnaire de paquets ultra-rapide d'Astral, √©crit en Rust. Il remplace pip, pip-tools, poetry et virtualenv.

```bash
# Installation
curl -LsSf https://astral.sh/uv/install.sh | sh

# Initialiser un projet
uv init

# Cr√©er un environnement virtuel
uv venv

# Activer l'environnement
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate     # Windows

# Installer les d√©pendances
uv pip install -r requirements.txt

# Ajouter une d√©pendance
uv add langchain langgraph

# Synchroniser l'environnement
uv pip sync requirements.txt
```

### Fichiers de Configuration

```
pyproject.toml          # Configuration projet et m√©tadonn√©es
requirements.txt        # D√©pendances lock√©es (production)
requirements-dev.txt    # D√©pendances d√©veloppement
.python-version         # Version Python fix√©e
uv.lock                 # Lockfile UV
```

---

## 2. Frameworks Agentiques

### 2.1 LangGraph ‚≠ê (Recommand√© Principal)

**Description** : Framework de LangChain pour orchestrer des workflows stateful avec graphes. Permet de cr√©er des agents complexes avec cycles, branches conditionnelles et state management.

**Use Cases** :
- Workflows multi-√©tapes avec d√©cisions conditionnelles
- Agents n√©cessitant de la m√©moire entre √©tapes
- Orchestration complexe avec retours en arri√®re

**Installation** :
```bash
uv add langgraph langchain-core langchain-anthropic
```

**Exemple d'architecture** :
```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated
import operator

class AgentState(TypedDict):
    messages: Annotated[list, operator.add]
    next_action: str
    context: dict

def create_workflow():
    workflow = StateGraph(AgentState)

    # N≈ìuds du graphe
    workflow.add_node("analyzer", analyze_input)
    workflow.add_node("executor", execute_task)
    workflow.add_node("validator", validate_output)

    # Edges conditionnels
    workflow.add_conditional_edges(
        "analyzer",
        route_decision,
        {
            "execute": "executor",
            "validate": "validator",
            "end": END
        }
    )

    workflow.set_entry_point("analyzer")
    return workflow.compile()
```

**Avantages** :
- Visualisation native des graphes
- State management robuste
- Support des checkpoints pour persistance
- Debugging facilit√©

---

### 2.2 CrewAI

**Description** : Framework pour orchestrer des √©quipes d'agents IA autonomes qui collaborent pour accomplir des t√¢ches complexes.

**Use Cases** :
- Agents avec r√¥les sp√©cifiques (analyst, writer, reviewer)
- Workflows collaboratifs type "√©quipe"
- D√©l√©gation de t√¢ches entre agents

**Installation** :
```bash
uv add crewai crewai-tools
```

**Exemple** :
```python
from crewai import Agent, Task, Crew, Process

# D√©finir les agents
researcher = Agent(
    role='Researcher',
    goal='Find and synthesize information',
    backstory='Expert data analyst',
    tools=[search_tool, scrape_tool],
    verbose=True
)

writer = Agent(
    role='Content Writer',
    goal='Create compelling content',
    backstory='Professional writer',
    tools=[write_tool],
    verbose=True
)

# D√©finir les t√¢ches
research_task = Task(
    description='Research about {topic}',
    agent=researcher,
    expected_output='Comprehensive research report'
)

write_task = Task(
    description='Write article based on research',
    agent=writer,
    expected_output='Published article'
)

# Cr√©er l'√©quipe
crew = Crew(
    agents=[researcher, writer],
    tasks=[research_task, write_task],
    process=Process.sequential,
    verbose=True
)

result = crew.kickoff(inputs={'topic': 'AI Agents'})
```

**Avantages** :
- API intuitive pour agents collaboratifs
- Gestion automatique de la d√©l√©gation
- Outils pr√©construits

---

### 2.3 AutoGen (Microsoft)

**Description** : Framework pour cr√©er des syst√®mes multi-agents conversationnels avec support human-in-the-loop.

**Use Cases** :
- Conversations multi-agents
- Code generation collaboratif
- Workflows n√©cessitant validation humaine

**Installation** :
```bash
uv add pyautogen
```

**Exemple** :
```python
from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager

# Agents
assistant = AssistantAgent(
    name="assistant",
    llm_config={"model": "gpt-4"}
)

user_proxy = UserProxyAgent(
    name="user_proxy",
    human_input_mode="TERMINATE",
    code_execution_config={"work_dir": "coding"}
)

# Group chat
groupchat = GroupChat(
    agents=[assistant, user_proxy],
    messages=[],
    max_round=10
)

manager = GroupChatManager(groupchat=groupchat)
```

---

### 2.4 LlamaIndex

**Description** : Framework pour construire des agents avec capacit√©s RAG (Retrieval-Augmented Generation) et acc√®s √† des donn√©es.

**Use Cases** :
- Agents n√©cessitant acc√®s √† des documents
- Question-answering sur donn√©es priv√©es
- Agents avec knowledge base

**Installation** :
```bash
uv add llama-index llama-index-llms-anthropic
```

**Exemple** :
```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import QueryEngineTool

# Charger des documents
documents = SimpleDirectoryReader("./data").load_data()
index = VectorStoreIndex.from_documents(documents)

# Cr√©er un outil de query
query_tool = QueryEngineTool.from_defaults(
    query_engine=index.as_query_engine(),
    name="knowledge_base",
    description="Useful for answering questions about documents"
)

# Cr√©er agent ReAct
agent = ReActAgent.from_tools([query_tool], verbose=True)
response = agent.chat("What are the key insights?")
```

---

### 2.5 Pydantic AI ‚≠ê

**Description** : Framework r√©cent de Pydantic pour cr√©er des agents type-safe avec validation stricte.

**Use Cases** :
- Agents n√©cessitant validation stricte des donn√©es
- Production systems avec garanties de types
- Int√©gration avec √©cosyst√®me Pydantic

**Installation** :
```bash
uv add pydantic-ai
```

**Exemple** :
```python
from pydantic import BaseModel
from pydantic_ai import Agent

class UserProfile(BaseModel):
    name: str
    age: int
    interests: list[str]

agent = Agent(
    'openai:gpt-4',
    result_type=UserProfile,
    system_prompt='Extract user profile from text'
)

result = agent.run_sync('John is 30 years old and loves coding and music')
print(result.data)  # UserProfile valid√©
```

---

### 2.6 Haystack

**Description** : Framework pour pipelines NLP, search et question-answering.

**Use Cases** :
- Pipelines de traitement de documents
- Search s√©mantique
- RAG pipelines customis√©s

**Installation** :
```bash
uv add haystack-ai
```

---

### üéØ Recommandations par Use Case

| Use Case | Framework Recommand√© |
|----------|---------------------|
| Workflows complexes stateful | **LangGraph** |
| √âquipes d'agents collaboratifs | **CrewAI** |
| Conversations multi-agents | **AutoGen** |
| RAG et knowledge bases | **LlamaIndex** |
| Validation stricte et type-safety | **Pydantic AI** |
| Pipelines NLP/Search | **Haystack** |

---

## 3. Model Context Protocol (MCP)

### Qu'est-ce que MCP ?

Le **Model Context Protocol** est un protocole ouvert d√©velopp√© par Anthropic pour permettre aux applications LLM de se connecter √† des sources de donn√©es externes et des outils de mani√®re standardis√©e.

### Architecture MCP

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LLM Client  ‚îÇ
‚îÇ (Claude,    ‚îÇ
‚îÇ  etc.)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îÇ MCP Protocol
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MCP Server  ‚îÇ
‚îÇ (Python)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Data Source ‚îÇ
‚îÇ (DB, API,   ‚îÇ
‚îÇ  Files...)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Impl√©mentation Python

**Installation** :
```bash
uv add mcp anthropic-mcp
```

**Cr√©er un MCP Server** :
```python
from mcp.server import Server
from mcp.types import Tool, TextContent

app = Server("my-mcp-server")

@app.list_tools()
async def list_tools() -> list[Tool]:
    return [
        Tool(
            name="search_database",
            description="Search in the database",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {"type": "string"}
                },
                "required": ["query"]
            }
        )
    ]

@app.call_tool()
async def call_tool(name: str, arguments: dict):
    if name == "search_database":
        query = arguments["query"]
        results = await search_db(query)
        return [TextContent(type="text", text=results)]
```

### MCPs Utiles √† Impl√©menter

1. **Filesystem MCP** : Acc√®s fichiers locaux
2. **Database MCP** : PostgreSQL, MongoDB, SQLite
3. **API Integration MCP** : REST APIs, GraphQL
4. **Google Workspace MCP** : Sheets, Docs, Drive
5. **Email MCP** : Gmail, Outlook
6. **Slack/Discord MCP** : Messaging platforms
7. **Git MCP** : Op√©rations Git
8. **Web Scraping MCP** : Beautiful Soup, Playwright

**Voir** : `directives/mcp-servers-guide.md` pour guide d√©taill√©

---

## 4. LLM Providers & Int√©grations

### 4.1 Anthropic Claude ‚≠ê

**Mod√®les Recommand√©s** :
- **claude-3-5-sonnet** : Meilleur rapport qualit√©/prix
- **claude-3-opus** : Tasks complexes
- **claude-3-haiku** : Rapide et √©conomique

```bash
uv add anthropic
```

```python
from anthropic import Anthropic

client = Anthropic(api_key="sk-ant-...")
response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello"}]
)
```

### 4.2 OpenAI

```bash
uv add openai
```

```python
from openai import OpenAI

client = OpenAI(api_key="sk-...")
response = client.chat.completions.create(
    model="gpt-4-turbo",
    messages=[{"role": "user", "content": "Hello"}]
)
```

### 4.3 LiteLLM ‚≠ê (Recommand√©)

**Abstraction unifi√©e** pour tous les LLM providers.

```bash
uv add litellm
```

```python
from litellm import completion

# Utiliser n'importe quel provider
response = completion(
    model="claude-3-5-sonnet-20241022",
    messages=[{"role": "user", "content": "Hello"}]
)

response = completion(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello"}]
)

response = completion(
    model="ollama/llama3",  # Local
    messages=[{"role": "user", "content": "Hello"}]
)
```

### 4.4 Autres Providers

- **Mistral AI** : `mistralai`
- **Groq** : `groq` (inf√©rence ultra-rapide)
- **Ollama** : Pour mod√®les locaux
- **Azure OpenAI** : `openai` avec endpoint Azure

---

## 5. Infrastructure & Outils

### 5.1 API Framework : FastAPI ‚≠ê

```bash
uv add fastapi uvicorn pydantic
```

**Exemple API agentique** :
```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class WorkflowRequest(BaseModel):
    workflow_id: str
    input_data: dict

@app.post("/workflows/execute")
async def execute_workflow(request: WorkflowRequest):
    result = await orchestrate_workflow(
        workflow_id=request.workflow_id,
        data=request.input_data
    )
    return {"status": "completed", "result": result}
```

### 5.2 State Management

**Redis** pour state distribu√© :
```bash
uv add redis asyncio-redis
```

**PostgreSQL** pour persistance :
```bash
uv add asyncpg sqlalchemy
```

**SQLite** pour d√©veloppement :
```python
import sqlite3
```

### 5.3 Task Queue : Celery

```bash
uv add celery redis
```

```python
from celery import Celery

app = Celery('tasks', broker='redis://localhost:6379/0')

@app.task
def run_agent_workflow(workflow_id: str, data: dict):
    # Ex√©cution asynchrone du workflow
    result = execute_workflow(workflow_id, data)
    return result
```

### 5.4 Linting & Formatting : Ruff ‚≠ê

```bash
uv add ruff --dev
```

**Configuration** (`pyproject.toml`) :
```toml
[tool.ruff]
line-length = 100
target-version = "py311"

[tool.ruff.lint]
select = ["E", "F", "I", "N", "W"]
ignore = ["E501"]
```

### 5.5 Testing

```bash
uv add pytest pytest-asyncio pytest-cov --dev
```

```python
import pytest
from agents.my_agent import MyAgent

@pytest.mark.asyncio
async def test_agent_execution():
    agent = MyAgent()
    result = await agent.execute({"task": "test"})
    assert result["status"] == "success"
```

### 5.6 Containerisation : Docker

**Dockerfile** :
```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Installer UV
RUN pip install uv

# Copier les d√©pendances
COPY requirements.txt .
RUN uv pip install -r requirements.txt --system

# Copier le code
COPY . .

CMD ["python", "-m", "execution.main"]
```

---

## 6. Patterns Agentiques

### 6.1 ReAct (Reasoning + Acting)

**Pattern** : L'agent raisonne (Thought) puis agit (Action) de mani√®re it√©rative.

```
Thought: I need to find the current temperature
Action: search_weather(city="Paris")
Observation: 15¬∞C, cloudy
Thought: Now I can answer the question
Final Answer: It's 15¬∞C in Paris
```

**Impl√©mentation** :
```python
class ReActAgent:
    def __init__(self, tools: list[Tool], llm):
        self.tools = tools
        self.llm = llm

    async def run(self, question: str) -> str:
        prompt = f"Question: {question}\n"

        for _ in range(max_iterations):
            response = await self.llm.generate(prompt)

            if "Final Answer:" in response:
                return extract_answer(response)

            if "Action:" in response:
                action, args = parse_action(response)
                observation = await self.execute_tool(action, args)
                prompt += f"Observation: {observation}\n"
```

### 6.2 Chain-of-Thought (CoT)

Demander au LLM de raisonner √©tape par √©tape avant de r√©pondre.

```python
system_prompt = """
You are a helpful assistant. When solving problems:
1. Break down the problem into steps
2. Think through each step carefully
3. Show your reasoning
4. Provide the final answer

Always use this format:
Reasoning: [your step-by-step thinking]
Answer: [final answer]
"""
```

### 6.3 Tool Calling Pattern

```python
from typing import Callable, Dict

class ToolRegistry:
    def __init__(self):
        self.tools: Dict[str, Callable] = {}

    def register(self, name: str):
        def decorator(func: Callable):
            self.tools[name] = func
            return func
        return decorator

    async def execute(self, name: str, **kwargs):
        if name not in self.tools:
            raise ValueError(f"Tool {name} not found")
        return await self.tools[name](**kwargs)

registry = ToolRegistry()

@registry.register("search_web")
async def search_web(query: str) -> str:
    # Impl√©mentation
    return results
```

### 6.4 Memory Management

**Short-term Memory** : Dans le contexte de conversation

```python
class ConversationMemory:
    def __init__(self, max_tokens: int = 4000):
        self.messages = []
        self.max_tokens = max_tokens

    def add_message(self, role: str, content: str):
        self.messages.append({"role": role, "content": content})
        self._trim_if_needed()

    def _trim_if_needed(self):
        # Garder seulement les N derniers messages
        if self.count_tokens() > self.max_tokens:
            self.messages = self.messages[-10:]
```

**Long-term Memory** : Stockage persistant

```python
from chromadb import Client

class LongTermMemory:
    def __init__(self):
        self.client = Client()
        self.collection = self.client.create_collection("agent_memory")

    def store(self, key: str, value: str, metadata: dict):
        self.collection.add(
            documents=[value],
            metadatas=[metadata],
            ids=[key]
        )

    def retrieve(self, query: str, n_results: int = 5):
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results
        )
        return results
```

### 6.5 Error Handling & Retry

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
async def call_llm_with_retry(prompt: str):
    try:
        return await llm.generate(prompt)
    except RateLimitError:
        logger.warning("Rate limit hit, retrying...")
        raise
    except Exception as e:
        logger.error(f"LLM call failed: {e}")
        raise
```

---

## 7. Observabilit√© & Monitoring

### 7.1 Structured Logging

```bash
uv add structlog
```

```python
import structlog

logger = structlog.get_logger()

logger.info(
    "workflow_started",
    workflow_id="wf_123",
    user_id="user_456",
    timestamp=datetime.now()
)
```

### 7.2 LLM Tracing : LangSmith / LangFuse

**LangSmith** (LangChain) :
```bash
export LANGCHAIN_TRACING_V2=true
export LANGCHAIN_API_KEY="lsv2_..."
```

**LangFuse** (Open-source alternative) :
```bash
uv add langfuse
```

```python
from langfuse import Langfuse

langfuse = Langfuse()

trace = langfuse.trace(name="agent_workflow")
span = trace.span(name="llm_call", input={"prompt": "..."})
# ... ex√©cution
span.end(output={"response": "..."})
```

### 7.3 M√©triques : Prometheus

```bash
uv add prometheus-client
```

```python
from prometheus_client import Counter, Histogram, start_http_server

workflow_counter = Counter(
    'workflows_total',
    'Total workflows executed',
    ['workflow_type', 'status']
)

workflow_duration = Histogram(
    'workflow_duration_seconds',
    'Workflow execution duration'
)

@workflow_duration.time()
async def execute_workflow(...):
    # ...
    workflow_counter.labels(
        workflow_type="data_processing",
        status="success"
    ).inc()
```

### 7.4 Dashboards : Grafana

Configurer Grafana pour visualiser les m√©triques Prometheus :
- Taux de succ√®s/√©chec des workflows
- Dur√©e d'ex√©cution
- Co√ªts LLM (tokens consomm√©s)
- Erreurs et retries

---

## 8. Migration N8N ‚Üí Code

### 8.1 Mapping des Concepts

| N8N Concept | √âquivalent Code |
|-------------|-----------------|
| **Workflow** | Python async function / LangGraph |
| **Node** | Tool / Function |
| **Trigger** | FastAPI endpoint / Scheduler |
| **Credentials** | Environment variables (.env) |
| **Variables** | Python variables / State dict |
| **IF Node** | Conditional logic (if/else) |
| **Switch Node** | Pattern matching (match/case) |
| **Loop** | for/while loops |
| **HTTP Request** | httpx / requests |
| **Code Node** | Python function inline |

### 8.2 Exemple de Migration

**N8N Workflow** :
```
[Webhook] ‚Üí [HTTP Request] ‚Üí [IF] ‚Üí [Send Email]
```

**√âquivalent Code** :
```python
from fastapi import FastAPI, Request
import httpx
from sendgrid import SendGridAPIClient
from sendgrid.helpers.mail import Mail

app = FastAPI()

@app.post("/webhook")
async def webhook_handler(request: Request):
    # 1. Recevoir webhook
    data = await request.json()

    # 2. HTTP Request
    async with httpx.AsyncClient() as client:
        response = await client.get(
            "https://api.example.com/data",
            params={"id": data["id"]}
        )
        result = response.json()

    # 3. Condition IF
    if result["status"] == "success":
        # 4. Send Email
        message = Mail(
            from_email='noreply@example.com',
            to_emails=data["email"],
            subject='Success',
            html_content='<strong>Done!</strong>'
        )
        sg = SendGridAPIClient(os.environ['SENDGRID_API_KEY'])
        sg.send(message)

    return {"status": "processed"}
```

### 8.3 Avantages du Code vs N8N

| Aspect | N8N | Code Python |
|--------|-----|-------------|
| **Version Control** | JSON export | Git natif ‚úÖ |
| **Testing** | Limit√© | Pytest complet ‚úÖ |
| **Debugging** | UI limit√©e | Debugger Python ‚úÖ |
| **Performance** | Overhead | Optimal ‚úÖ |
| **Scalabilit√©** | Single instance | Horizontal scaling ‚úÖ |
| **Type Safety** | Aucune | Pydantic ‚úÖ |
| **Agents IA** | Basique | Frameworks complets ‚úÖ |
| **Complexit√©** | UI limitante | Illimit√© ‚úÖ |
| **Cost** | Self-hosted | Self-hosted ‚úÖ |

---

## 9. Standards de D√©veloppement

### 9.1 Structure de Projet

```
workflows-boilerplate/
‚îú‚îÄ‚îÄ directives/           # SOPs et sp√©cifications
‚îÇ   ‚îú‚îÄ‚îÄ TECHNICAL_SPECS.md
‚îÇ   ‚îú‚îÄ‚îÄ workflow_*.md
‚îÇ   ‚îî‚îÄ‚îÄ mcp-servers-guide.md
‚îú‚îÄ‚îÄ execution/            # Code d'ex√©cution
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ exceptions.py
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_agent.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ specific_agents.py
‚îÇ   ‚îú‚îÄ‚îÄ workflows/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ workflow_implementations.py
‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tool_registry.py
‚îÇ   ‚îú‚îÄ‚îÄ mcp_servers/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ custom_servers.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ api_clients.py
‚îÇ       ‚îî‚îÄ‚îÄ validators.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îî‚îÄ‚îÄ e2e/
‚îú‚îÄ‚îÄ .tmp/                 # Fichiers temporaires (gitignore)
‚îú‚îÄ‚îÄ .env                  # Variables d'environnement
‚îú‚îÄ‚îÄ pyproject.toml        # Config UV et d√©pendances
‚îú‚îÄ‚îÄ requirements.txt      # D√©pendances lock√©es
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```

### 9.2 Conventions de Code

**Naming** :
- `snake_case` : fonctions, variables, fichiers
- `PascalCase` : classes
- `UPPER_CASE` : constantes
- Pr√©fixes : `_private`, `__very_private`

**Type Hints** (obligatoire) :
```python
def process_data(
    input_data: dict[str, Any],
    config: Config,
    timeout: float = 30.0
) -> ProcessingResult:
    ...
```

**Docstrings** (Google style) :
```python
def execute_workflow(workflow_id: str, data: dict) -> dict:
    """
    Ex√©cute un workflow agentique.

    Args:
        workflow_id: Identifiant unique du workflow
        data: Donn√©es d'entr√©e pour le workflow

    Returns:
        R√©sultat de l'ex√©cution avec status et outputs

    Raises:
        WorkflowNotFoundError: Si le workflow n'existe pas
        ValidationError: Si les donn√©es sont invalides
    """
    pass
```

### 9.3 Configuration (.env)

```bash
# LLM APIs
ANTHROPIC_API_KEY=sk-ant-...
OPENAI_API_KEY=sk-...
MISTRAL_API_KEY=...

# Infrastructure
REDIS_URL=redis://localhost:6379
DATABASE_URL=postgresql://user:pass@localhost/db

# MCP Servers
MCP_SERVER_PORT=3000

# Observability
LANGSMITH_API_KEY=...
LANGFUSE_PUBLIC_KEY=...
LANGFUSE_SECRET_KEY=...

# Application
ENVIRONMENT=development
LOG_LEVEL=INFO
MAX_RETRIES=3
TIMEOUT=30
```

### 9.4 Pre-commit Hooks

```bash
uv add pre-commit --dev
```

`.pre-commit-config.yaml` :
```yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.6
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format

  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
```

---

## üéØ Checklist de D√©marrage

Lors de la cr√©ation d'un nouveau workflow agentique :

- [ ] Cr√©er une directive dans `directives/workflow_name.md`
- [ ] D√©finir les Pydantic models pour validation
- [ ] Impl√©menter les agents dans `execution/agents/`
- [ ] Cr√©er les outils n√©cessaires dans `execution/tools/`
- [ ] Configurer le logging structur√©
- [ ] Ajouter les tests (unit + integration)
- [ ] Configurer le monitoring (m√©triques + traces)
- [ ] Documenter dans la directive les learnings
- [ ] Dockeriser si n√©cessaire
- [ ] Configurer les variables d'environnement

---

## üìö Ressources

### Documentation Officielle
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [CrewAI](https://docs.crewai.com/)
- [AutoGen](https://microsoft.github.io/autogen/)
- [LlamaIndex](https://docs.llamaindex.ai/)
- [Pydantic AI](https://ai.pydantic.dev/)
- [MCP Protocol](https://modelcontextprotocol.io/)
- [UV Documentation](https://docs.astral.sh/uv/)

### Outils
- [LangSmith](https://smith.langchain.com/) - Tracing LLM
- [LangFuse](https://langfuse.com/) - Open-source LLM observability
- [Ruff](https://docs.astral.sh/ruff/) - Linter/Formatter

---

**Version** : 1.0.0
**Derni√®re mise √† jour** : 2026-01-09
**Maintenu par** : L'√©quipe du projet
